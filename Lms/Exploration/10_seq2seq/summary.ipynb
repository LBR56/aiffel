{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 번역기를 만들어보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자연어처리의 핵심적 어플리케이션인 기계번역 모델에 대해 알아보고, 신경망 기계번역을 가능하게 했던 seq2seq 모델 구성을 이해하고 직접 만들어 본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 들어가며\n",
    "- 기계 번역 이야기\n",
    "- 시퀀스 처리하는 RNN\n",
    "- seq2seq\n",
    "- 교사 강요(teacher forcing)\n",
    "- 단어 수준 vs 문자 수준\n",
    "- 번역기 만들기\n",
    "  - 데이터 전처리\n",
    "  - 모델 훈련하기\n",
    "  - 모델 테스트하기\n",
    "- 프로젝트 : 단어 Level로 번역기 업그레이드하기\n",
    "- 프로젝트 제출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 들어가며"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기계 번역 이야기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시퀀스 처리하는 RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 교사 강요(teacher forcing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어 수준 vs 문자 수준"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 번역기 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 194513\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "      <th>cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>153740</th>\n",
       "      <td>Some of my best friends are policemen.</td>\n",
       "      <td>Certains de mes meilleurs amis sont policiers.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82548</th>\n",
       "      <td>We buy stationery in bulk.</td>\n",
       "      <td>Nous achetons la papeterie en gros.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4042</th>\n",
       "      <td>Tom's lying.</td>\n",
       "      <td>Tom ment.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41745</th>\n",
       "      <td>I can't kiss you now.</td>\n",
       "      <td>Je ne peux pas vous embrasser maintenant.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138463</th>\n",
       "      <td>Hurry up, or you'll miss the train.</td>\n",
       "      <td>Dépêchez-vous, ou vous manquerez le train.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           eng  \\\n",
       "153740  Some of my best friends are policemen.   \n",
       "82548               We buy stationery in bulk.   \n",
       "4042                              Tom's lying.   \n",
       "41745                    I can't kiss you now.   \n",
       "138463     Hurry up, or you'll miss the train.   \n",
       "\n",
       "                                                   fra  \\\n",
       "153740  Certains de mes meilleurs amis sont policiers.   \n",
       "82548              Nous achetons la papeterie en gros.   \n",
       "4042                                         Tom ment.   \n",
       "41745        Je ne peux pas vous embrasser maintenant.   \n",
       "138463      Dépêchez-vous, ou vous manquerez le train.   \n",
       "\n",
       "                                                       cc  \n",
       "153740  CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
       "82548   CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "4042    CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "41745   CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "138463  CC-BY 2.0 (France) Attribution: tatoeba.org #1...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "file_path = 'data/fra.txt'\n",
    "lines = pd.read_csv(file_path, names=['eng', 'fra', 'cc'], sep='\\t')\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5) #샘플 5개 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2790</th>\n",
       "      <td>Be watchful.</td>\n",
       "      <td>Fais attention.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26832</th>\n",
       "      <td>We had no secrets.</td>\n",
       "      <td>Nous n'avions pas de secrets.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19747</th>\n",
       "      <td>Let's go bowling.</td>\n",
       "      <td>Allons faire du bowling.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22712</th>\n",
       "      <td>Do you have a dog?</td>\n",
       "      <td>As-tu un chien ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16203</th>\n",
       "      <td>Tom looks angry.</td>\n",
       "      <td>Tom a l'air furieux.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      eng                            fra\n",
       "2790         Be watchful.                Fais attention.\n",
       "26832  We had no secrets.  Nous n'avions pas de secrets.\n",
       "19747   Let's go bowling.       Allons faire du bowling.\n",
       "22712  Do you have a dog?               As-tu un chien ?\n",
       "16203    Tom looks angry.           Tom a l'air furieux."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines[['eng', 'fra']][:50000] # 5만개 샘플 사용\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 50000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7183</th>\n",
       "      <td>I don't drink.</td>\n",
       "      <td>\\t Je m'abstiens de boire. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22884</th>\n",
       "      <td>Everyone got sick.</td>\n",
       "      <td>\\t Tout le monde se mit à être malade. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4574</th>\n",
       "      <td>Does it hurt?</td>\n",
       "      <td>\\t Cela fait-il mal ? \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21823</th>\n",
       "      <td>Why did you quit?</td>\n",
       "      <td>\\t Pourquoi as-tu démissionné ? \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47023</th>\n",
       "      <td>Says who? \"Says me.\"</td>\n",
       "      <td>\\t « Dixit qui ? » « Dixit moi. » \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        eng                                        fra\n",
       "7183         I don't drink.              \\t Je m'abstiens de boire. \\n\n",
       "22884    Everyone got sick.  \\t Tout le monde se mit à être malade. \\n\n",
       "4574          Does it hurt?                   \\t Cela fait-il mal ? \\n\n",
       "21823     Why did you quit?         \\t Pourquoi as-tu démissionné ? \\n\n",
       "47023  Says who? \"Says me.\"       \\t « Dixit qui ? » « Dixit moi. » \\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 시작 토큰과 종료 토큰 추가\n",
    "sos_token = '\\t'\n",
    "eos_token = '\\n'\n",
    "lines.fra = lines.fra.apply(lambda x : '\\t '+ x + ' \\n')\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[19, 3, 8], [19, 3, 8], [19, 3, 8]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_tokenizer = Tokenizer(char_level=True)   # 문자 단위로 Tokenizer를 생성합니다. \n",
    "eng_tokenizer.fit_on_texts(lines.eng)               # 50000개의 행을 가진 eng의 각 행에 토큰화를 수행\n",
    "input_text = eng_tokenizer.texts_to_sequences(lines.eng)    # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "input_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10, 1, 19, 5, 1, 31, 1, 11],\n",
       " [10, 1, 15, 5, 12, 16, 29, 2, 14, 1, 11],\n",
       " [10, 1, 26, 9, 8, 28, 2, 1, 31, 1, 11]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fra_tokenizer = Tokenizer(char_level=True)   # 문자 단위로 Tokenizer를 생성합니다. \n",
    "fra_tokenizer.fit_on_texts(lines.fra)                 # 50000개의 행을 가진 fra의 각 행에 토큰화를 수행\n",
    "target_text = fra_tokenizer.texts_to_sequences(lines.fra)     # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "target_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어장의 크기 : 53\n",
      "프랑스어 단어장의 크기 : 73\n"
     ]
    }
   ],
   "source": [
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 시퀀스의 최대 길이 22\n",
      "프랑스어 시퀀스의 최대 길이 76\n"
     ]
    }
   ],
   "source": [
    "max_eng_seq_len = max([len(line) for line in input_text])\n",
    "max_fra_seq_len = max([len(line) for line in target_text])\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 50000\n",
      "영어 단어장의 크기 : 53\n",
      "프랑스어 단어장의 크기 : 73\n",
      "영어 시퀀스의 최대 길이 22\n",
      "프랑스어 시퀀스의 최대 길이 76\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플의 수 :',len(lines))\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = input_text\n",
    "# 종료 토큰 제거\n",
    "decoder_input = [[ char for char in line if char != fra_tokenizer.word_index[eos_token] ] for line in target_text] \n",
    "# 시작 토큰 제거\n",
    "decoder_target = [[ char for char in line if char != fra_tokenizer.word_index[sos_token] ] for line in target_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (50000, 22)\n",
      "프랑스어 입력데이터의 크기(shape) : (50000, 76)\n",
      "프랑스어 출력데이터의 크기(shape) : (50000, 76)\n"
     ]
    }
   ],
   "source": [
    "encoder_input = pad_sequences(encoder_input, maxlen = max_eng_seq_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen = max_fra_seq_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen = max_fra_seq_len, padding='post')\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (50000, 22, 53)\n",
      "프랑스어 입력데이터의 크기(shape) : (50000, 76, 73)\n",
      "프랑스어 출력데이터의 크기(shape) : (50000, 76, 73)\n"
     ]
    }
   ],
   "source": [
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 학습데이터의 크기(shape) : (50000, 22, 53)\n",
      "프랑스어 학습 입력데이터의 크기(shape) : (50000, 76, 73)\n",
      "프랑스어 학습 출력데이터의 크기(shape) : (50000, 76, 73)\n"
     ]
    }
   ],
   "source": [
    "n_of_val = 3000\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('영어 학습데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 학습 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 학습 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "print('⏳')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-16 09:40:51.460606: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# 입력 텐서 생성.\n",
    "encoder_inputs = Input(shape=(None, eng_vocab_size))\n",
    "# hidden size가 256인 인코더의 LSTM 셀 생성\n",
    "encoder_lstm = LSTM(units = 256, return_state = True)\n",
    "# 디코더로 전달할 hidden state, cell state를 리턴. encoder_outputs은 여기서는 불필요.\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "# hidden state와 cell state를 다음 time step으로 전달하기 위해서 별도 저장.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 텐서 생성.\n",
    "decoder_inputs = Input(shape=(None, fra_vocab_size))\n",
    "# hidden size가 256인 인코더의 LSTM 셀 생성\n",
    "decoder_lstm = LSTM(units = 256, return_sequences = True, return_state=True)\n",
    "# decoder_outputs는 모든 time step의 hidden state\n",
    "decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state = encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_softmax_layer = Dense(fra_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, None, 53)]   0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, None, 73)]   0           []                               \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 256),        317440      ['input_1[0][0]']                \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, None, 256),  337920      ['input_2[0][0]',                \n",
      "                                 (None, 256),                     'lstm[0][1]',                   \n",
      "                                 (None, 256)]                     'lstm[0][2]']                   \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, None, 73)     18761       ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 674,121\n",
      "Trainable params: 674,121\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "368/368 [==============================] - 160s 425ms/step - loss: 0.8892 - val_loss: 0.7682\n",
      "Epoch 2/50\n",
      "368/368 [==============================] - 140s 382ms/step - loss: 0.5423 - val_loss: 0.6237\n",
      "Epoch 3/50\n",
      "368/368 [==============================] - 143s 387ms/step - loss: 0.4533 - val_loss: 0.5552\n",
      "Epoch 4/50\n",
      "368/368 [==============================] - 120s 326ms/step - loss: 0.3957 - val_loss: 0.5008\n",
      "Epoch 5/50\n",
      "368/368 [==============================] - 125s 339ms/step - loss: 0.3590 - val_loss: 0.4694\n",
      "Epoch 6/50\n",
      "368/368 [==============================] - 128s 349ms/step - loss: 0.3326 - val_loss: 0.4431\n",
      "Epoch 7/50\n",
      "368/368 [==============================] - 123s 333ms/step - loss: 0.3125 - val_loss: 0.4230\n",
      "Epoch 8/50\n",
      "368/368 [==============================] - 114s 311ms/step - loss: 0.2965 - val_loss: 0.4080\n",
      "Epoch 9/50\n",
      "368/368 [==============================] - 119s 322ms/step - loss: 0.2832 - val_loss: 0.4015\n",
      "Epoch 10/50\n",
      "368/368 [==============================] - 119s 324ms/step - loss: 0.2721 - val_loss: 0.3927\n",
      "Epoch 11/50\n",
      "368/368 [==============================] - 119s 323ms/step - loss: 0.2623 - val_loss: 0.3865\n",
      "Epoch 12/50\n",
      "368/368 [==============================] - 119s 323ms/step - loss: 0.2538 - val_loss: 0.3771\n",
      "Epoch 13/50\n",
      "368/368 [==============================] - 244s 665ms/step - loss: 0.2462 - val_loss: 0.3746\n",
      "Epoch 14/50\n",
      "368/368 [==============================] - 101s 274ms/step - loss: 0.2394 - val_loss: 0.3704\n",
      "Epoch 15/50\n",
      "368/368 [==============================] - 118s 321ms/step - loss: 0.2332 - val_loss: 0.3706\n",
      "Epoch 16/50\n",
      "368/368 [==============================] - 142s 386ms/step - loss: 0.2275 - val_loss: 0.3636\n",
      "Epoch 17/50\n",
      "368/368 [==============================] - 159s 433ms/step - loss: 0.2224 - val_loss: 0.3652\n",
      "Epoch 18/50\n",
      "368/368 [==============================] - 158s 430ms/step - loss: 0.2174 - val_loss: 0.3617\n",
      "Epoch 19/50\n",
      "368/368 [==============================] - 133s 360ms/step - loss: 0.2128 - val_loss: 0.3658\n",
      "Epoch 20/50\n",
      "368/368 [==============================] - 130s 352ms/step - loss: 0.2085 - val_loss: 0.3586\n",
      "Epoch 21/50\n",
      "368/368 [==============================] - 139s 377ms/step - loss: 0.2046 - val_loss: 0.3591\n",
      "Epoch 22/50\n",
      "368/368 [==============================] - 134s 365ms/step - loss: 0.2007 - val_loss: 0.3590\n",
      "Epoch 23/50\n",
      "368/368 [==============================] - 111s 303ms/step - loss: 0.1971 - val_loss: 0.3557\n",
      "Epoch 24/50\n",
      "368/368 [==============================] - 108s 294ms/step - loss: 0.1938 - val_loss: 0.3630\n",
      "Epoch 25/50\n",
      "368/368 [==============================] - 111s 302ms/step - loss: 0.1905 - val_loss: 0.3589\n",
      "Epoch 26/50\n",
      "368/368 [==============================] - 125s 339ms/step - loss: 0.1874 - val_loss: 0.3655\n",
      "Epoch 27/50\n",
      "368/368 [==============================] - 128s 347ms/step - loss: 0.1845 - val_loss: 0.3639\n",
      "Epoch 28/50\n",
      "368/368 [==============================] - 118s 320ms/step - loss: 0.1817 - val_loss: 0.3668\n",
      "Epoch 29/50\n",
      "368/368 [==============================] - 127s 346ms/step - loss: 0.1789 - val_loss: 0.3672\n",
      "Epoch 30/50\n",
      "368/368 [==============================] - 119s 323ms/step - loss: 0.1764 - val_loss: 0.3719\n",
      "Epoch 31/50\n",
      "368/368 [==============================] - 105s 287ms/step - loss: 0.1738 - val_loss: 0.3712\n",
      "Epoch 32/50\n",
      "368/368 [==============================] - 107s 290ms/step - loss: 0.1713 - val_loss: 0.3774\n",
      "Epoch 33/50\n",
      "368/368 [==============================] - 113s 307ms/step - loss: 0.1691 - val_loss: 0.3725\n",
      "Epoch 34/50\n",
      "368/368 [==============================] - 107s 290ms/step - loss: 0.1668 - val_loss: 0.3722\n",
      "Epoch 35/50\n",
      "368/368 [==============================] - 311s 847ms/step - loss: 0.1645 - val_loss: 0.3777\n",
      "Epoch 36/50\n",
      "368/368 [==============================] - 132s 358ms/step - loss: 0.1624 - val_loss: 0.3787\n",
      "Epoch 37/50\n",
      "368/368 [==============================] - 112s 304ms/step - loss: 0.1604 - val_loss: 0.3816\n",
      "Epoch 38/50\n",
      "368/368 [==============================] - 103s 281ms/step - loss: 0.1582 - val_loss: 0.3812\n",
      "Epoch 39/50\n",
      "368/368 [==============================] - 104s 282ms/step - loss: 0.1564 - val_loss: 0.3863\n",
      "Epoch 40/50\n",
      "368/368 [==============================] - 112s 305ms/step - loss: 0.1545 - val_loss: 0.3871\n",
      "Epoch 41/50\n",
      "368/368 [==============================] - 103s 279ms/step - loss: 0.1526 - val_loss: 0.3898\n",
      "Epoch 42/50\n",
      "368/368 [==============================] - 103s 279ms/step - loss: 0.1507 - val_loss: 0.3961\n",
      "Epoch 43/50\n",
      "368/368 [==============================] - 103s 279ms/step - loss: 0.1491 - val_loss: 0.3919\n",
      "Epoch 44/50\n",
      "368/368 [==============================] - 103s 280ms/step - loss: 0.1474 - val_loss: 0.3962\n",
      "Epoch 45/50\n",
      "368/368 [==============================] - 103s 279ms/step - loss: 0.1457 - val_loss: 0.4007\n",
      "Epoch 46/50\n",
      "368/368 [==============================] - 523s 1s/step - loss: 0.1440 - val_loss: 0.4042\n",
      "Epoch 47/50\n",
      "368/368 [==============================] - 115s 312ms/step - loss: 0.1424 - val_loss: 0.4029\n",
      "Epoch 48/50\n",
      "368/368 [==============================] - 106s 288ms/step - loss: 0.1410 - val_loss: 0.4037\n",
      "Epoch 49/50\n",
      "368/368 [==============================] - 133s 362ms/step - loss: 0.1394 - val_loss: 0.4080\n",
      "Epoch 50/50\n",
      "368/368 [==============================] - 110s 300ms/step - loss: 0.1379 - val_loss: 0.4118\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14026c940>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "          validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size=128, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 테스트하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None, 53)]        0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 [(None, 256),             317440    \n",
      "                              (None, 256),                       \n",
      "                              (None, 256)]                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 317,440\n",
      "Trainable params: 317,440\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model = Model(inputs = encoder_inputs, outputs = encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이전 time step의 hidden state를 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "# 이전 time step의 cell state를 저장하는 텐서\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "# 이전 time step의 hidden state와 cell state를 하나의 변수에 저장\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# decoder_states_inputs를 현재 time step의 초기 상태로 사용.\n",
    "# 구체적인 동작 자체는 def decode_sequence()에 구현.\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state = decoder_states_inputs)\n",
    "# 현재 time step의 hidden state와 cell state를 하나의 변수에 저장.\n",
    "decoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, None, 73)]   0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, None, 256),  337920      ['input_2[0][0]',                \n",
      "                                 (None, 256),                     'input_3[0][0]',                \n",
      "                                 (None, 256)]                     'input_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, None, 73)     18761       ['lstm_1[1][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 356,681\n",
      "Trainable params: 356,681\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng2idx = eng_tokenizer.word_index\n",
    "fra2idx = fra_tokenizer.word_index\n",
    "idx2eng = eng_tokenizer.index_word\n",
    "idx2fra = fra_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_seq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/main/Desktop/temp/Lms/Exploration/10_seq2seq/summary.ipynb 셀 37\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/main/Desktop/temp/Lms/Exploration/10_seq2seq/summary.ipynb#X51sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m states_value \u001b[39m=\u001b[39m encoder_model\u001b[39m.\u001b[39mpredict(input_seq)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_seq' is not defined"
     ]
    }
   ],
   "source": [
    "states_value = encoder_model.predict(input_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1, 1, fra_vocab_size))\n",
    "    target_seq[0, 0, fra2idx['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = idx2fra[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_fra_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1, fra_vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for seq_index in [3,50,100,300,1001]: # 입력 문장의 인덱스 (자유롭게 선택해 보세요)\n",
    "    input_seq = encoder_input[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', lines.eng[seq_index])\n",
    "    print('정답 문장:', lines.fra[seq_index][1:len(lines.fra[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 프로젝트 : 단어 Level로 번역기 업그레이드하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 프로젝트 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
